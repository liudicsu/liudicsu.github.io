---
layout:     post
title:      "朴素贝叶斯无监督学习"
subtitle:   "EM算法求解"
date:       2015-01-29 12:00:00
author:     "liudicsu"
header-img: "img/post-bg-2015.jpg"
tags:
    - 机器学习
    - 无监督学习
    - EM算法
    - 朴素贝叶斯 
    - 统计学习方法
---

<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

学了EM算法总感觉模模糊糊，翻了几遍书总感觉还是没完全弄明白，看到书后面有个习题使用em算法构造朴素贝叶的无监督学习算法。拿过来练练手，顺便发出来。
下面先将需要使用的符号进行说明。（注：这些符号说明来自 《统计学习方法》 第4章 李航著）<br>
输入空间\( \chi \subseteq \mathbf{R}^{n} \)为n维向量的集合空间，输出空间为类标记集合\(\gamma =\{c_1 ,c_2 , ...,c_k \}\)
输入体征向量\(X\in \chi\),输出类标记为\(Y \in \gamma , P(X,Y)\)是\(X,Y\)的联合概率分布，训练数据集：
$$T=\{(x_1 , y_1 ), (x_2 ,y_2 ),..., (x_N , y_N ) \} $$是由\(P(x,y)\)的独立同分布产生。
在这里与朴素贝叶不同的是无监督的朴素贝叶斯学习中，训练数据中的\(y_i , i=1,...,N\)是未知的。
但是在这里我学习的目的是一致的，通过数据集学习联合概率分布\(P(X,Y)\),具体的学习以下先验概率及条件概率分布：
先验概率分布：
$$P(Y=c_k ),k=1,2,...,K$$
条件概率分布：
$$
            \begin{align}
                P(X=x|Y=c_k )&=P(X^{(1)} =x^{(1)} ,...,X^{(n)}= x^{(n)} |Y=c_k )     \\
                &=\prod_{i=1}^{n}P(X^{(i)} =x^{(i)}|Y=c_k)     \\
                k=1,2,...,K
            \end{align}
$$
从上面的公式我们可以看出最终我们要求的是其实是：$$P(X^{(i)} =x^{(i)}|Y=c_k)$$ $$i =1,2,..,n, k=1,2,...,K-1$$
在这里我们已经定义完需要的符号。
在em算法中有隐变量与观察变量，在这里数据集中的每一个数据\(y_i\)就是隐变量，\(x_i\)就是观测数据。在这里们的em算法的模型参数\(\lambda =\{P(Y=c_k ), P(X^{(i)}=1|Y=c_k )\} ,i =1,2,..,n, k=1,2,...,K-1 \),\(P(Y=c_K) =1-\sum_{k=1}^{K-1}\left(P(Y=c_k)\right)\)在后面将\(P(X^{(i)}=1|Y=c_k )\)简写为\(P_{k}^{i}\)
em算法中分为两步E步骤 M步骤。对 \(\lambda\)进行初始赋值，下面\(\lambda\)的初始值与每次迭代后的值都表示为\(\lambda^{'}\)
E步骤：直接给出，不进行推导了。\(Q(y_i = c_k; \lambda^{'} )\)代表是给定参数\(\lambda^{'}\)情况下\(y_{i}=c_{k}\)的概率。
$$
            \begin{align}
                Q(y_i = c_k; \lambda^{'} ) &= \frac{P(X=x_i , y_i=c_k;\lambda^{'} )}{P(X=x_i;\lambda^{'})}     \\
                                           &= \frac{P(X=x_i , y_i=c_k;\lambda^{'} )}{\sum_{k=1}^{K}P(X_i =x_i, y_i =c_k ;\lambda^{'})}
            \end{align}
$$

M步骤，也直接给出需要最大化的公式
$$
            \begin{align}
                L(\lambda)&=\sum_{i=1}^{N}\sum_{k=1}^{K}\left[Q(y_{i}=c_{k};\lambda^{'})ln\left(\frac{P(y_{i}=c_{k},X_{i}=x_{i};\lambda)}{Q(y_{i}=c_{k};\lambda^{'})}\right)\right]     \\

&=\sum_{i=1}^{N}\sum_{k=1}^{K}\left[Q(y_{i}=c_{k};\lambda^{'})ln\left(\frac{P(y_{i}=c_{k};\lambda)P(X_{i}=x_{i}|y_{i}=c_{k};\lambda)}{Q(y_{i}=c_{k};\lambda^{'})}\right)\right]     \\

&=\sum_{i=1}^{N}\sum_{k=1}^{K}\left[Q(y_{i}=c_{k};\lambda^{'})ln\left(\frac{P(y_{i}=c_{k};\lambda)\prod_{j=1}^{n}P(X_{i}^{j}=x_{i}^{j}|y_{i}=c_{k};\lambda)}{Q(y_{i}=c_{k};\lambda^{'})}\right)\right]     \\

&=\sum_{i=1}^{N}\sum_{k=1}^{K}\left[Q(y_{i}=c_{k};\lambda^{'})ln\left(\frac{P(y_{i}=c_{k};\lambda)\prod_{j=1}^{n}\left[ P_{k}^{j}(i)^{x_{i}^{j}}(1-P_{k}^{j}(i))^{1-x_{i}^{j}}\right ]}{Q(y_{i}=c_{k};\lambda^{'})}\right)\right]     \\

&=\sum_{i=1}^{N}\sum_{k=1}^{K}\left[Q(y_{i}=c_{k};\lambda^{'})ln\left(\frac{P(Y=c_{k})\prod_{j=1}^{n}\left[ P_{k}^{j}(i)^{x_{i}^{j}}(1-P_{k}^{j}(i))^{1-x_{i}^{j}}\right ]}{Q(y_{i}=c_{k};\lambda^{'})}\right)\right]    \\

						\end{align}

$$
