---
layout:     post
title:      "贝叶斯学习"
subtitle:   "最大后验假设 极大似然估计"
date:       2015-02-01 12:00:00
author:     "liudicsu"
header-img: "img/post-bg-2015.jpg"
tags:
    - 机器学习
    - 贝叶斯学习
    - 最大后验假设 
    - 极大似然估计
    - 统计学习方法
---

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

贝叶斯学习  最大后验假设(MAP) 极大似然估计(ML) <br>
前提：在这里每个观测值之间在是相互独立的， 即X的取值与D没有关系，至于对应假设空间相关，但是确切的假设空间位置，只能通过D寻找最有可能的假设空间。<br>
给定观测数据 D，以及可能的假设空间\( h_{1}, h_{2},....,h_{n}\)，预测下一个可能的观测X。<br>

贝叶斯学习：对每个假设空间产生的观测值X的概率进行计算，然后根据一定权重的比例加权求和，达到观测值X的的概率。如果要预测X为多少，对X所有的可能值进行概率求解，去概率最大的那个值作为结果。 

$$
 \begin{align}
P(X|D) &= \sum_{i=1}^{n}P(X,h_{i}|D)\\
&= \sum_{i=1}^{n}P(X,h_{i}|D)\\
&= \sum_{i=1}^{n}P(X|h_{i},D)P(h_{i}|D)
   \end{align}
$$

在上面的等式中\(P(h_{i}|D)\)相当于是上面提到的权重

$$
 \begin{align}
P(h_{i}|D) &= \frac{P(D|h_{i})p(h_{i})}{P(D)}\\
					 &= \alpha P(D|h_{i})p(h_{i})
 \end{align}
$$

由于计算时对于所有的假设空间，当观测数据已知时\(P(D)\)的值是相等的，所以用\(\alpha\)表示。
由上面的两个公式可以看出，贝叶斯学习时每个假设空间的先验概率\(P(h_i)\)是已知的，以及\(P(D|h_{i})\)是能过通过已知条件计算出来。

理论上来说贝叶斯学习得到的结果是最优的，但是计算量太大。试想一下如果可能的假设是无穷多个，贝叶斯学习没法计算。


最大后验假设(MAP)
最大后验假设通过观测数据\(D\)求得最有可能假设\(\hat{h}\),然后直接使用\(\hat{h}\)去对X进行预测。
$$
\hat{h}=\max_{h_{i}}P(h_{i}|D)
$$
$$
P(X|D) \approx P(X|\hat{h})
$$

极大似然估计(ML)
极大似然估计求得一个使观测数据\(D\)概率最大的假设\(\hat{h}\),然后通过\(\hat{h}\)去对X进行预测。

$$
\hat{h}=\max_{h_{i}}P(D|h_{i})
$$
$$
P(X|D) \approx P(X|\hat{h})
$$


总结：从上面的内容可以看出，贝叶斯学习，最大后验假设(MAP)，极大似然估计(ML) 计算量依次递减，贝叶斯学习退化为最大后验假设(MAP)，最大后验假设(MAP)进一步退化得到极大似然估计(ML)
